<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VS-Net: Voting with Segmentation for Visual Localization</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-4">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>VS-Net: Voting with Segmentation for Visual Localization</h2>
            <h4 style="color:#5a6268;">CVPR 2021</h4>
            <hr>
            <h6> <a href="https://drinkingcoder.github.io/" target="_blank">Zhaoyang Huang</a><sup>1,2*</sup>, 
                Han Zhou<sup>1*</sup>, Yijin Li<sup>1</sup>, Bangbang Yang<sup>1</sup>, Yan Xu<sup>2</sup>, 
                <br>
                <a href="https://xzhou.me" target="_blank">Xiaowei Zhou</a><sup>1</sup> 
                Hujun Bao<sup>1</sup>,
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>1</sup>,
                <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a><sup>2, 3</sup></h6>
            <p><sup>1</sup>State Key Lab of CAD & CG, Zhejiang University &nbsp;&nbsp;  <br>
                <sup>2</sup>CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong &nbsp;&nbsp; <br>
                <sup>3</sup> School of CST, Xidian University &nbsp;&nbsp;
                <br>
                <sup>*</sup> denotes equal contributions
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/pdf/" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/zju3dv/VS-Net" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code</a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://zjueducn-my.sharepoint.com/:f:/g/personal/pengsida_zju_edu_cn/Eo9zn4x_xcZKmYHZNjzel7gBdWf_d4m-pISHhPWB-GZBYw?e=Hf4mz7" role="button">
                    <i class="fa fa-database"></i> Data</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""> -->
                  <!-- <source src="images/monocular.m4v" type="video/mp4"> -->
              <!-- </video> -->
              <!-- <br><br> -->
          <p class="text-justify">Visual localization is of great importance in robotics and computer vision. 
            Recently, scene coordinate regression based methods have shown good performance in visual localization 
            in small static scenes. However, it still estimates camera poses from many inferior scene coordinates. 
            To address this problem, we propose a novel visual localization framework that establishes 2D-to-3D 
            correspondences between the query image and the 3D map with a series of learnable scene-specific landmarks. 
            In the landmark generation stage, the 3D surfaces of the target scene are oversegmented into mosaic patches 
            whose centers are regarded as the scene-specific landmarks. 
            To robustly and accurately recover the scene-specific landmarks, 
            we propose the Voting with Segmentation Network (VS-Net) to segment the pixels into different landmark patches 
            with a segmentation branch and estimate the landmark locations within each patch with a landmark location voting 
            branch. Since the number of landmarks in a scene may reach up to 5000, 
            training a segmentation network with such a large number of classes is both computation and memory costly 
            for the commonly used cross-entropy loss. We propose a novel prototype-based triplet loss with hard negative mining, 
            which is able to train semantic segmentation networks with a large number of labels efficiently. 
            Our proposed VS-Net is extensively tested on multiple public benchmarks and can outperform state-of-the-art visual 
            localization methods.</p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- pipeline overview -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Pipeline overview</h3>
            <hr style="margin-top:0px">
            <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6>
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/VS-Framework2.png" alt="VS Architechture">
          <p class="text-justify">There are two decoder branches respectively predicting a landmark segmentation map and a pixel-wise voting map, from which we can detect the location and labels of landmarks. After establishing 2D-3D correspondences according to the landmark labels, we can estimate the 6-DoF camera pose with a standard RANSAC-PnP.  </p>
            <!-- <hr> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Landmark generation from the proposed voting-by-segmentation algorithm</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/VotingbySegmentation.png" alt="VS landmark generation">
          <p class="text-justify"> In the localization stage, pixels that are predicted to have the same landmark label in the landmark segmentation map are grouped together and we estimate its corresponding landmark location by computing the intersection of the landmark directional votes from the predicted voting map, which is dubbed the voting-by-segmentation algorithm. The landmarks generated by our voting-by-segmentation algorithm achieve high accuracy and robustness against distracting factors because we can accurately detect landmark locations by filtering disturbed pixel votes (pointed by the red arrow) and further reject unstable regions (pointed by the green arrow) in advance by checking the voting consistency.</p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Prototype-based triplet loss for segmentation</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/Proto.png" alt="VS prototype-based">
          <p class="text-justify"> In the localization stage, pixels that are predicted to have the same landmark label in the landmark segmentation map are grouped together and we estimate its corresponding landmark location by computing the intersection of the landmark directional votes from the predicted voting map, which is dubbed the voting-by-segmentation algorithm. The landmarks generated by our voting-by-segmentation algorithm achieve high accuracy and robustness against distracting factors because we can accurately detect landmark locations by filtering disturbed pixel votes (pointed by the red arrow) and further reject unstable regions (pointed by the green arrow) in advance by checking the voting consistency.</p>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- Comparison with Scene Coordinate -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with scene coordinates</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6> -->
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/RepErr.jpg" alt="VS Comparison">
          <p class="text-justify"> Reprojection errors of 2D-to-3D correspondences of scene coordinates and scene-specific landmarks.   (a) Thequery image.  (b) The reprojection errors of dense scene coordinates predicted by the regression-only network [25].  (c) Thereprojection errors of scene-specific landmarks and their surrounding patches by the proposed method.  Pixels belonging tothe same landmark are painted with the same color representing the landmarkâ€™ reprojection error. The white pixels in (c) are filetered by our voting-by-segmentation algorithm.  </p>
            <!-- <hr> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with general features</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6> -->
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/FeatureMatching.png" alt="FeatureMatching Comparison">
          <p class="text-justify"> All the matches have been filtered by epipolar geometry with RANSAC. R2D2 is not able to figure out more than three correspondences, which can not be applied to the following fundamental matrix estimation, so we do not present it in the figures.  Even after RANSAC, there are many erroneous matches in SIFT and D2-Net when the baseline is large because the textures are locally similar. We highlight some of them with red circles. By contrast, our landmark association keeps good robustness and accuracy under such a significant viewpoint changing. </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- overview video -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Qualitative localization performance</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/BPCAMeBCE-8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
                <iframe width="560" height="315" src="https://www.youtube.com/embed/5WLEyyLdxAs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          <!-- <p class="text-justify">  </p> -->
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- h36m results -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Results on Human3.6M dataset</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/h36m-1.m4v" type="video/mp4">
            </video>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/h36m-2.m4v" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- multiview results -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with state-of-the-art methods on sparse multi-view videos</h3>
            <hr style="margin-top:0px">
            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Novel view synthesis of dynamic human</h4>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/4-view-video-1.m4v" type="video/mp4">
            </video>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/4-view-video-2.m4v" type="video/mp4">
            </video>

            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Novel view synthesis of frame 1</h4>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/4-view-first-frame.m4v" type="video/mp4">
            </video>

            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">3D reconstruction</h4>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/4-view-recon.m4v" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- monocular results -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Results on monocular videos</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/monocular-1.m4v" type="video/mp4">
            </video>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="images/monocular-2.m4v" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{huang2021vs-net,
  title={VS-Net: Voting with Segmentation for Visual Localization},
  author={Huang, Zhaoyang and Zhou, Han and Li, Yijin and Yang, Bangbang and Xu, Yan and Zhou, Xiaowei and Bao, Hujun and Zhang, Guofeng and Li, Hongsheng},
  booktitle={CVPR},
  year={2021}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
