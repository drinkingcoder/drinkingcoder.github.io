<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-180733097-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-180733097-1');
</script>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VS-Net: Voting with Segmentation for Visual Localization</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-4">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>VS-Net: Voting with Segmentation for Visual Localization</h2>
            <h4 style="color:#5a6268;">CVPR 2021</h4>
            <hr>
            <h6> <a href="https://drinkingcoder.github.io/" target="_blank">Zhaoyang Huang</a><sup>1,2*</sup>, 
                Han Zhou<sup>1*</sup>, Yijin Li<sup>1</sup>, Bangbang Yang<sup>1</sup>, Yan Xu<sup>2</sup>, 
                <br>
                <a href="https://xzhou.me" target="_blank">Xiaowei Zhou</a><sup>1</sup> 
                Hujun Bao<sup>1</sup>,
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>1</sup>,
                <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a><sup>2, 3</sup></h6>
            <p><sup>1</sup>State Key Lab of CAD & CG, Zhejiang University &nbsp;&nbsp;  <br>
                <sup>2</sup>CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong &nbsp;&nbsp; <br>
                <sup>3</sup> School of CST, Xidian University &nbsp;&nbsp;
                <br>
                <sup>*</sup> denotes equal contributions
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="https://arxiv.org/abs/2105.10886" role="button">Paper</a>  -->
                <!-- <a class="btn btn-secondary btn-lg" href="" role="button">Code</a>  -->
                <!-- <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2105.10886" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/zju3dv/VS-Net" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code</a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://zjueducn-my.sharepoint.com/:f:/g/personal/pengsida_zju_edu_cn/Eo9zn4x_xcZKmYHZNjzel7gBdWf_d4m-pISHhPWB-GZBYw?e=Hf4mz7" role="button">
                    <i class="fa fa-database"></i> Data</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""> -->
                  <!-- <source src="images/monocular.m4v" type="video/mp4"> -->
              <!-- </video> -->
              <!-- <br><br> -->
          <p class="text-justify">Visual localization is of great importance in robotics and computer vision. 
            Recently, scene coordinate regression based methods have shown good performance in visual localization 
            in small static scenes. However, it still estimates camera poses from many inferior scene coordinates. 
            To address this problem, we propose a novel visual localization framework that establishes 2D-to-3D 
            correspondences between the query image and the 3D map with a series of learnable scene-specific landmarks. 
            In the landmark generation stage, the 3D surfaces of the target scene are oversegmented into mosaic patches 
            whose centers are regarded as the scene-specific landmarks. 
            To robustly and accurately recover the scene-specific landmarks, 
            we propose the Voting with Segmentation Network (VS-Net) to segment the pixels into different landmark patches 
            with a segmentation branch and estimate the landmark locations within each patch with a landmark location voting 
            branch. Since the number of landmarks in a scene may reach up to 5000, 
            training a segmentation network with such a large number of classes is both computation and memory costly 
            for the commonly used cross-entropy loss. We propose a novel prototype-based triplet loss with hard negative mining, 
            which is able to train semantic segmentation networks with a large number of labels efficiently. 
            Our proposed VS-Net is extensively tested on multiple public benchmarks and can outperform state-of-the-art visual 
            localization methods.</p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Motivation of Scene-specific Landmarks</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/motivation.png" alt="motivation" width="80%">
          <p class="text-justify"> Estimating camera poses by the Pnp algorithm doesn’t require 2D-to-3D dense correspondences. Instead, sparse, uniformly distributed, and accurate correspondences can better benefit localization. Object pose estimation establishes 2D-3D correspondences by identifying predefined keypoints on the 3D model, which can produce sparse, uniformly distributed, and accurate correspondences. Inspired by them, we propose to define landmarks on the given 3D model, which are referred to as scene-specific landmarks, and train a neural network to identify such scene-specific landmarks in 2D images. Furthermore, visual localization is faced with a much larger model and should process a large number of landmarks. We propose a voting with segmentation neural network to address this problem.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- pipeline overview -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Pipeline overview</h3>
            <hr style="margin-top:0px">
            <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6>
            <video id="demo" width="75%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="images/demo.mp4" type="video/mp4">
            </video>
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/VS-Framework2.png" alt="VS Architechture">
          <p class="text-justify"> There are two decoder branches respectively predicting a landmark segmentation map and a pixel-wise direction voting map. We identify the location and labels of landmarks with these two maps by counting the intersection of pixel votes grouped by the segmentation map. After establishing 2D-3D correspondences according to the landmark labels, we can estimate the 6-DoF camera pose with a standard RANSAC-PnP.  </p>
            <!-- <hr> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Identifying Scene-specific Landmark with VS-Net</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/VotingbySegmentation.png" alt="VS landmark generation">
          <p class="text-justify"> We detect the landmarks with the voting-intersection based RANSAC algorithm.</p>
           <p class="text-justify" style="text-indent: 40px"> 1) Filter out disturbed pixels for landmark localization by RANSAC. Estimating the location from the remaining pixel votes achieves high accuracy and robustness against distracting factors.  </p>
           <p class="text-justify" style="text-indent: 40px">2) Reject unreliable landmark locations that have too few inlier votes to ensure the accuracy of correspondences involved in the follow-up pose estimation
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Prototype-based triplet loss for segmentation</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/Proto.png" alt="VS prototype-based" width="50%">
          <p class="text-justify"> Cross-entropy loss is computation and memory costly when the number of labels is large so we train the segmentation map by the proposed prototype-based triplet loss:</p>
           <p class="text-justify" style="text-indent: 40px"> 1) Maintain a learnable prototype set that contains a prototype feature for each landmark (label)</p>
           <p class="text-justify" style="text-indent: 40px">2) Push a feature embedding corresponding to a pixel to its target prototype</p>
           <p class="text-justify" style="text-indent: 40px"> 3) Pull the feature embedding from top-k non-corresponding prototypes rather than all non-corresponding prototypes </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Comparison with Scene Coordinate -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with scene coordinates</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6> -->
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/RepErr.jpg" alt="VS Comparison">
          <p class="text-justify"> Reprojection errors of 2D-to-3D correspondences of scene coordinates and scene-specific landmarks.   (a) The query image.  (b) The reprojection errors of dense scene coordinates predicted by the regression-only network.  (c) The reprojection errors of scene-specific landmarks and their surrounding patches by the proposed method.  Pixels belonging to the same landmark are painted with the same color representing the landmark’ reprojection error. The white pixels in (c) are filetered by our voting-by-segmentation algorithm. Our scene-specific landmarks achieve much lower reporjection error compared with dense scene coordinate regression. </p>
            <!-- <hr> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Comparison with SfM Map -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with SfM Map</h3>
            <hr style="margin-top:0px">
                <img class="img-fluid" src="images/MapComparison.png" alt="VS Comparison">
          <p class="text-justify">   </p>
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with general features</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6> -->
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/FeatureMatching.png" alt="FeatureMatching Comparison">
          <p class="text-justify"> All the matches have been filtered by epipolar geometry with RANSAC. R2D2 is not able to figure out more than three correspondences, which can not be applied to the following fundamental matrix estimation, so we do not present it in the figures.  Even after RANSAC, there are many erroneous matches in SIFT and D2-Net when the baseline is large because the textures are locally similar. We highlight some of them with red circles. By contrast, our landmark association keeps good robustness and accuracy under such a significant viewpoint changing because VS-Net mines scene bias well. </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Quantitative Comparison</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> VS-Net: Voting with Segmentation for Visual Localization  </h6> -->
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/vltable.png" alt="Quantitative Comparison">
          <p> Our VS-Net outperforms previous visual localization methods on the Microsoft 7-Scenes dataset and the Cambridge Landmarks dataset. </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- overview video -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Qualitative localization performance</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/BPCAMeBCE-8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
                <iframe width="560" height="315" src="https://www.youtube.com/embed/5WLEyyLdxAs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          <!-- <p class="text-justify">  </p> -->
        </div>
      </div>
    </div>
  </section>
  <br>



  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{huang2021vs-net,
  title={VS-Net: Voting with Segmentation for Visual Localization},
  author={Huang, Zhaoyang and Zhou, Han and Li, Yijin and Yang, Bangbang and Xu, Yan and Zhou, Xiaowei and Bao, Hujun and Zhang, Guofeng and Li, Hongsheng},
  booktitle={CVPR},
  year={2021}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
